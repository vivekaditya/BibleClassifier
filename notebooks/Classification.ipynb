{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf Vectorizer Based KNN Classifier\n",
    "This notebook creates tf-idf vectors out of the dataset provided. Scikit-learn has a built in Tf-Idf \n",
    "implementation. we will also use NLTK's tokenizer and stemmer to preprocess the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  \n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.metrics as metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import *\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/bible_data_set_full.csv',sep='\\t')\n",
    "np.random.seed(seed=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df['testment']\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(df,y,test_size = 0.3,random_state = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0       citation      book  chapter  verse  \\\n",
      "15313       15313    Psalms 88:5    Psalms       88      5   \n",
      "6575         6575     Judges 3:7    Judges        3      7   \n",
      "16270       16270   Psalms 140:7    Psalms      140      7   \n",
      "9424         9424  1 Kings 20:16   1 Kings       20     16   \n",
      "19445       19445  Jeremiah 21:5  Jeremiah       21      5   \n",
      "\n",
      "                                                    text  testment  \\\n",
      "15313  Free among the dead, like the slain that lie i...         1   \n",
      "6575   And the children of Israel did evil in the sig...         1   \n",
      "16270  O GOD the Lord, the strength of my salvation, ...         1   \n",
      "9424   And they went out at noon. But Benhadad was dr...         1   \n",
      "19445  And I myself will fight against you with an ou...         1   \n",
      "\n",
      "                                   nounTextBlob  \\\n",
      "15313  {'free', 'thou rememberest', 'thy hand'}   \n",
      "6575        {'baalim', 'lord', 'god', 'israel'}   \n",
      "16270                      {'thou hast', 'god'}   \n",
      "9424                               {'benhadad'}   \n",
      "19445             {'great wrath', 'strong arm'}   \n",
      "\n",
      "                                                nounNLTK  \n",
      "15313                         {'slain', 'hand', 'grave'}  \n",
      "6575   {'God', 'LORD', 'sight', 'Baalim', 'Israel', '...  \n",
      "16270  {'strength', 'O', 'thou', 'Lord', 'salvation',...  \n",
      "9424   {'kings', 'drunk', 'noon', 'thirty', 'Benhadad...  \n",
      "19445          {'wrath', 'hand', 'arm', 'anger', 'fury'}  \n"
     ]
    }
   ],
   "source": [
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flags\n",
    "\n",
    "Flags to turn on and off removal of puntuations, removal of stopwords and stemming words using porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PUNCTUATION_FLAG = True\n",
    "STOPWORDS_FLAG = True\n",
    "STEMMING_FLAG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simpleTokenize = lambda doc: doc.lower().split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    lowers = text.lower() \n",
    "    \n",
    "    if PUNCTUATION_FLAG:\n",
    "        no_punctuation = lowers.translate(string.punctuation)\n",
    "        tokens = nltk.word_tokenize(no_punctuation)\n",
    "    else:\n",
    "        tokens = nltk.word_tokenize(lowers)\n",
    "    \n",
    "    if STOPWORDS_FLAG:\n",
    "        filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    else:\n",
    "        filtered = tokens\n",
    "        \n",
    "    if STEMMING_FLAG:\n",
    "        stemmer = PorterStemmer()\n",
    "        final_tokens = stem_tokens(filtered, stemmer) \n",
    "    else:\n",
    "        final_tokens = token\n",
    "        \n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and create TfIdf Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "tfidf_text_matrix = text_tfidf.fit_transform(X_train['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding cosine distance between light and dark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.]]\n"
     ]
    }
   ],
   "source": [
    "# print(text_tfidf.transform([\"light\"]))\n",
    "print(1 - cosine_similarity(text_tfidf.transform([\"light\"]),text_tfidf.transform([\"dark\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=10, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=10,weights=\"distance\",n_jobs=-1)\n",
    "neigh.fit(text_tfidf.transform(X_train['text']), Y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting and calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9331\n",
      "9331\n"
     ]
    }
   ],
   "source": [
    "y_pred = neigh.predict(text_tfidf.transform(X_test['text']))\n",
    "print(len(y_pred))  \n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.90324986807030827, 0.85200864312256452, 0.87336497947152658, None)\n",
      "(0.90933447647626198, 0.90933447647626198, 0.90933447647626198, None)\n",
      "(0.90837843646732186, 0.90933447647626198, 0.90626770255106426, None)\n",
      "[[1756  635]\n",
      " [ 211 6729]]\n"
     ]
    }
   ],
   "source": [
    "actual_labels = Y_test\n",
    "predicted_labels = y_pred \n",
    "print(metrics.precision_recall_fscore_support(actual_labels, predicted_labels, average='macro'))\n",
    "print(metrics.precision_recall_fscore_support(actual_labels, predicted_labels, average='micro'))\n",
    "print(metrics.precision_recall_fscore_support(actual_labels, predicted_labels, average='weighted'))\n",
    "print(metrics.confusion_matrix(actual_labels, predicted_labels,np.unique(actual_labels))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
